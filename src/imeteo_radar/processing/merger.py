#!/usr/bin/env python3
"""
Radar Data Merger

Merges radar data from multiple sources (SHMU, DWD) into unified composites.
"""

import numpy as np
import cv2
from typing import Dict, Any, List, Optional, Tuple
from ..core.base import lonlat_to_mercator, mercator_to_lonlat

class RadarMerger:
    """Merges radar data from multiple sources"""
    
    def __init__(self):
        self.merge_strategies = {
            'average': self._average_merge,
            'priority': self._priority_merge, 
            'weighted': self._weighted_merge,
            'max': self._max_merge
        }
        
    def merge_sources(self, 
                     timestamp_data: Dict[str, List[Dict[str, Any]]], 
                     source_data: Dict[str, Dict[str, Any]],
                     strategy: str = 'average',
                     target_resolution: Optional[Tuple[int, int]] = None) -> Optional[Dict[str, Any]]:
        """
        Merge radar data from multiple sources for a specific timestamp
        
        Args:
            timestamp_data: Data for specific timestamp from each source
            source_data: Complete source metadata and extent info
            strategy: Merging strategy ('average', 'priority', 'weighted', 'max')
            target_resolution: Target grid resolution (height, width)
            
        Returns:
            Merged radar data dictionary or None if merge fails
        """
        
        if len(timestamp_data) < 2:
            print("⚠️  Need at least 2 sources to merge")
            return None
            
        print(f"🔀 Merging {len(timestamp_data)} sources using '{strategy}' strategy")
        
        try:
            # Determine target grid and extent
            target_extent, target_shape = self._compute_target_grid(
                source_data, target_resolution
            )
            
            print(f"🎯 Target extent: {target_extent['wgs84']}")
            print(f"📐 Target shape: {target_shape}")
            
            # Regrid all sources to target grid
            regridded_data = {}
            for source_name, files in timestamp_data.items():
                print(f"📊 Regridding {source_name} data...")
                
                # For now, use first file from each source (TODO: handle multiple products)
                file_data = files[0] if files else None
                if not file_data:
                    continue
                    
                regridded = self._regrid_to_target(
                    file_data, target_extent, target_shape
                )
                
                if regridded is not None:
                    regridded_data[source_name] = regridded
                    
            if len(regridded_data) < 2:
                print("❌ Failed to regrid enough sources for merging")
                return None
                
            # Store target coordinates for weighted merge
            self._last_target_extent = target_extent
            if 'lons' in target_extent and 'lats' in target_extent:
                self._last_target_coords = (target_extent['lons'], target_extent['lats'])
                
            # Apply merging strategy
            merge_func = self.merge_strategies.get(strategy, self._average_merge)
            merged_data = merge_func(regridded_data)
            
            # Create merged result
            timestamp = list(timestamp_data.values())[0][0]['timestamp']
            
            return {
                'data': merged_data,
                'coordinates': {
                    'lons': target_extent['lons'],
                    'lats': target_extent['lats']
                },
                'metadata': {
                    'product': 'MERGED',
                    'quantity': 'DBZH',  # Assume reflectivity for now
                    'timestamp': timestamp,
                    'source': 'MULTI',
                    'sources': list(regridded_data.keys()),
                    'merge_strategy': strategy,
                    'units': 'dBZ',
                    'nodata_value': np.nan
                },
                'extent': target_extent,
                'dimensions': target_shape,
                'timestamp': timestamp
            }
            
        except Exception as e:
            print(f"❌ Merge failed: {e}")
            return None
            
    def _compute_target_grid(self, 
                           source_data: Dict[str, Dict[str, Any]], 
                           target_resolution: Optional[Tuple[int, int]]) -> Tuple[Dict[str, Any], Tuple[int, int]]:
        """Compute target grid that encompasses all sources"""
        
        # Find combined extent
        all_extents = [data['extent']['wgs84'] for data in source_data.values()]
        
        combined_west = min(extent['west'] for extent in all_extents)
        combined_east = max(extent['east'] for extent in all_extents)
        combined_south = min(extent['south'] for extent in all_extents)
        combined_north = max(extent['north'] for extent in all_extents)
        
        # Use target resolution or compute from highest resolution source
        if target_resolution:
            target_shape = target_resolution
        else:
            # Use resolution from source with most pixels
            max_pixels = 0
            best_shape = (1000, 1000)  # Default
            
            for data in source_data.values():
                shape = data['extent'].get('grid_size', [1000, 1000])
                pixels = shape[0] * shape[1]
                if pixels > max_pixels:
                    max_pixels = pixels
                    best_shape = tuple(shape)
                    
            target_shape = best_shape
            
        # Create coordinate arrays
        lons = np.linspace(combined_west, combined_east, target_shape[1])
        lats = np.linspace(combined_north, combined_south, target_shape[0])
        
        # Convert to mercator
        x_min, y_min = lonlat_to_mercator(combined_west, combined_south)
        x_max, y_max = lonlat_to_mercator(combined_east, combined_north)
        
        target_extent = {
            'wgs84': {
                'west': combined_west,
                'east': combined_east,
                'south': combined_south,
                'north': combined_north
            },
            'mercator': {
                'x_min': x_min,
                'x_max': x_max, 
                'y_min': y_min,
                'y_max': y_max,
                'bounds': [x_min, y_min, x_max, y_max]
            },
            'lons': lons,
            'lats': lats
        }
        
        return target_extent, target_shape
        
    def _regrid_to_target(self, 
                         file_data: Dict[str, Any],
                         target_extent: Dict[str, Any], 
                         target_shape: Tuple[int, int]) -> Optional[np.ndarray]:
        """Regrid source data to target grid"""
        
        try:
            source_data = file_data['data']
            source_coords = file_data['coordinates']
            
            # Use fast cv2.remap interpolation
            return self._regrid_data_fast(
                source_data, source_coords, target_extent, target_shape
            )
            
        except Exception as e:
            print(f"❌ Regridding failed: {e}")
            return None
    
    def _regrid_data_fast(self, source_data: np.ndarray, source_coords: Dict[str, np.ndarray],
                         target_extent: Dict[str, np.ndarray], target_shape: Tuple[int, int]) -> Optional[np.ndarray]:
        """Fast regridding using OpenCV remap (8-15x faster than scipy)"""
        try:
            print(f"🚀 Fast regridding: {source_data.shape} → {target_shape}")
            
            # Handle invalid data
            valid_mask = np.isfinite(source_data)
            if not np.any(valid_mask):
                print("⚠️  No valid data to regrid")
                return None
            
            # Get coordinate arrays
            source_lats = source_coords['lats']
            source_lons = source_coords['lons']
            target_lats = target_extent['lats'] 
            target_lons = target_extent['lons']
            
            # Handle both 1D and 2D coordinate arrays
            # DWD now provides 2D coordinates due to projection handling
            if source_lats.ndim == 2:
                # For 2D arrays, extract edges for orientation check
                lat_edge = source_lats[:, 0]  # Left edge
                lon_edge = source_lons[0, :]  # Top edge
            else:
                # For 1D arrays, use directly
                lat_edge = source_lats
                lon_edge = source_lons
                
            # Ensure data is in correct orientation (ascending coordinates) only for 1D
            if source_lats.ndim == 1:
                if source_lats[0] > source_lats[-1]:
                    source_lats = source_lats[::-1]
                    source_data = source_data[::-1, :]
                
                if source_lons[0] > source_lons[-1]:
                    source_lons = source_lons[::-1]
                    source_data = source_data[:, ::-1]
            
            # Create coordinate mapping for cv2.remap
            if source_lats.ndim == 2:
                # For 2D coordinates, use scipy's nearest neighbor search
                # This is slower but necessary for proper projection handling
                from scipy.spatial import cKDTree
                
                # Create source coordinate pairs
                source_points = np.column_stack([source_lons.flatten(), source_lats.flatten()])
                
                # Create target coordinate pairs
                target_lons_2d, target_lats_2d = np.meshgrid(target_lons, target_lats)
                target_points = np.column_stack([target_lons_2d.flatten(), target_lats_2d.flatten()])
                
                # Build KD-tree for fast nearest neighbor search
                tree = cKDTree(source_points)
                
                # Find nearest neighbors
                distances, indices = tree.query(target_points, k=1)
                
                # Convert flat indices to 2D indices
                source_shape = source_lats.shape
                source_y_idx = indices // source_shape[1]
                source_x_idx = indices % source_shape[1]
                
                # Create mapping arrays
                map_y = source_y_idx.reshape(target_shape).astype(np.float32)
                map_x = source_x_idx.reshape(target_shape).astype(np.float32)
                
            else:
                # For 1D coordinates, use the original linear interpolation
                # target_shape is (height, width) = (lat_count, lon_count)
                map_y, map_x = np.meshgrid(
                    np.interp(target_lats, source_lats, np.arange(len(source_lats))),
                    np.interp(target_lons, source_lons, np.arange(len(source_lons))),
                    indexing='ij'
                )
            
            # Convert to float32 for cv2
            source_data_f32 = source_data.astype(np.float32)
            map_x_f32 = map_x.astype(np.float32) if map_x.dtype != np.float32 else map_x
            map_y_f32 = map_y.astype(np.float32) if map_y.dtype != np.float32 else map_y
            
            # Create validity mask before remapping
            valid_mask = np.isfinite(source_data_f32)
            
            # Use large negative value instead of NaN for border (cv2 doesn't handle NaN properly)
            INVALID_VALUE = -9999.0
            
            # Use cv2.remap for fast interpolation
            interpolated = cv2.remap(
                source_data_f32,
                map_x_f32, 
                map_y_f32,
                interpolation=cv2.INTER_LINEAR,
                borderMode=cv2.BORDER_CONSTANT,
                borderValue=INVALID_VALUE
            )
            
            # Also remap the validity mask to track which pixels should be valid
            valid_mask_f32 = valid_mask.astype(np.float32)
            interpolated_mask = cv2.remap(
                valid_mask_f32,
                map_x_f32,
                map_y_f32,
                interpolation=cv2.INTER_NEAREST,  # Use nearest neighbor for mask
                borderMode=cv2.BORDER_CONSTANT,
                borderValue=0.0
            )
            
            # Apply proper masking - set pixels to NaN where:
            # 1. They were remapped from invalid source pixels
            # 2. They got the border fill value
            # 3. The interpolated mask indicates invalidity
            final_mask = (interpolated_mask < 0.5) | (interpolated <= INVALID_VALUE) | (interpolated >= 1000)
            interpolated[final_mask] = np.nan
            
            # Additional quality control - clip to valid meteorological range
            interpolated = np.clip(interpolated, -35, 85)
            
            print(f"✅ Fast regridded to shape {target_shape}, "
                  f"valid pixels: {np.sum(np.isfinite(interpolated))}")
            
            return interpolated.astype(np.float64)  # Convert back to expected dtype
            
        except Exception as e:
            print(f"❌ Fast regridding failed: {e}")
            # Fallback to scipy method
            print("📉 Falling back to scipy interpolation...")
            return self._regrid_data(source_data, source_coords, target_extent, target_shape)
            
    def _average_merge(self, regridded_data: Dict[str, np.ndarray]) -> np.ndarray:
        """Improved average merge strategy with quality control and range clipping"""
        
        print("📊 Applying average merge strategy")
        
        # Stack all arrays
        arrays = list(regridded_data.values())
        stacked = np.stack(arrays, axis=0)
        
        # Quality control - remove outliers before averaging
        # Detect pixels where sources disagree too much (likely errors)
        with np.errstate(invalid='ignore', divide='ignore'):
            # Calculate standard deviation across sources for each pixel
            std_values = np.nanstd(stacked, axis=0)
            mean_values = np.nanmean(stacked, axis=0)
            
            # Identify pixels where standard deviation is suspiciously high
            # (more than 15 dBZ difference between sources indicates possible errors)
            outlier_mask = std_values > 15.0
            
            # For outlier pixels, use median instead of mean (more robust)
            merged = np.where(outlier_mask, 
                             np.nanmedian(stacked, axis=0),
                             mean_values)
        
        # Post-processing quality control
        # 1. Clip to valid meteorological range for radar reflectivity
        merged = np.clip(merged, -32.0, 80.0)
        
        # 2. Remove isolated pixels (likely noise)
        # Simple 3x3 majority filter - if a pixel has fewer than 2 valid neighbors, set to NaN
        from scipy import ndimage
        valid_mask = ~np.isnan(merged)
        neighbor_count = ndimage.generic_filter(
            valid_mask.astype(float), 
            np.sum, 
            size=3, 
            mode='constant', 
            cval=0.0
        )
        # Keep pixels that have at least 2 valid neighbors (including themselves)
        isolated_mask = neighbor_count < 3
        merged[isolated_mask] = np.nan
        
        # 3. Final validation - ensure no extreme values
        extreme_mask = (merged < -35) | (merged > 85)
        merged[extreme_mask] = np.nan
            
        valid_pixels = np.sum(~np.isnan(merged))
        print(f"✅ Average merge: {valid_pixels} valid pixels")
        
        return merged
        
    def _priority_merge(self, regridded_data: Dict[str, np.ndarray]) -> np.ndarray:
        """Priority merge - use first source, fill gaps with others"""
        
        print("🎯 Applying priority merge strategy")
        
        sources = list(regridded_data.keys())
        print(f"📋 Priority order: {' > '.join(sources)}")
        
        # Start with first source
        merged = regridded_data[sources[0]].copy()
        
        # Fill gaps with subsequent sources
        for source in sources[1:]:
            mask = np.isnan(merged)
            merged[mask] = regridded_data[source][mask]
            
        valid_pixels = np.sum(~np.isnan(merged))
        print(f"✅ Priority merge: {valid_pixels} valid pixels")
        
        return merged
        
    def _weighted_merge(self, regridded_data: Dict[str, np.ndarray]) -> np.ndarray:
        """Weighted merge based on distance from radar centers and data quality"""
        
        print("⚖️  Applying weighted merge strategy")
        
        # Radar center coordinates (approximate)
        radar_centers = {
            'shmu': (19.15, 48.55),  # Bratislava, Slovakia (approximate)
            'dwd': (10.0, 51.5)      # Central Germany (approximate)
        }
        
        # Get target grid coordinates (assuming first source has coordinate info)
        first_source = list(regridded_data.keys())[0]
        if hasattr(self, '_last_target_coords'):
            target_lons, target_lats = self._last_target_coords
        else:
            # Fallback - create coordinate grids for target extent
            target_extent = self._last_target_extent if hasattr(self, '_last_target_extent') else {
                'west': 2.5, 'east': 23.8, 'south': 45.5, 'north': 56.0
            }
            target_shape = list(regridded_data.values())[0].shape
            
            lons = np.linspace(target_extent['west'], target_extent['east'], target_shape[1])
            lats = np.linspace(target_extent['north'], target_extent['south'], target_shape[0])
            target_lons, target_lats = np.meshgrid(lons, lats)
        
        # Calculate distance-based weights
        total_weights = np.zeros(target_lons.shape)
        weighted_sum = np.zeros(target_lons.shape)
        
        for source_name, data in regridded_data.items():
            if source_name in radar_centers:
                radar_lon, radar_lat = radar_centers[source_name]
                
                # Calculate distance from radar center (in degrees, approximately)
                distance = np.sqrt((target_lons - radar_lon)**2 + (target_lats - radar_lat)**2)
                
                # Inverse distance weighting (closer = higher weight)
                # Add small constant to avoid division by zero
                weights = 1.0 / (distance + 0.1)
                
                # Quality-based adjustment
                # Give higher weight to data that seems more reliable
                valid_mask = np.isfinite(data)
                
                # Reduce weight for extreme values (likely errors)
                extreme_mask = (data < -30) | (data > 75)
                weights = np.where(extreme_mask, weights * 0.1, weights)
                
                # Only use weights where data is valid
                masked_weights = np.where(valid_mask, weights, 0.0)
                masked_data = np.where(valid_mask, data, 0.0)
                
                weighted_sum += masked_data * masked_weights
                total_weights += masked_weights
            else:
                # If no position known, use equal weight
                valid_mask = np.isfinite(data)
                uniform_weight = 1.0
                
                masked_weights = np.where(valid_mask, uniform_weight, 0.0)
                masked_data = np.where(valid_mask, data, 0.0)
                
                weighted_sum += masked_data * masked_weights
                total_weights += masked_weights
        
        # Calculate weighted average
        with np.errstate(invalid='ignore', divide='ignore'):
            merged = np.divide(weighted_sum, total_weights, 
                             out=np.full_like(weighted_sum, np.nan), 
                             where=total_weights > 0)
        
        # Apply quality control
        merged = np.clip(merged, -32.0, 80.0)
        
        # Remove isolated pixels
        from scipy import ndimage
        valid_mask = ~np.isnan(merged)
        neighbor_count = ndimage.generic_filter(
            valid_mask.astype(float), np.sum, size=3, mode='constant', cval=0.0
        )
        isolated_mask = neighbor_count < 3
        merged[isolated_mask] = np.nan
        
        valid_pixels = np.sum(~np.isnan(merged))
        print(f"✅ Weighted merge: {valid_pixels} valid pixels")
        
        return merged
        
    def _max_merge(self, regridded_data: Dict[str, np.ndarray]) -> np.ndarray:
        """Maximum value merge - take highest reflectivity"""
        
        print("📈 Applying maximum merge strategy")
        
        # Stack all arrays
        arrays = list(regridded_data.values())
        stacked = np.stack(arrays, axis=0)
        
        # Calculate max ignoring NaN values
        with np.errstate(invalid='ignore'):
            merged = np.nanmax(stacked, axis=0)
            
        valid_pixels = np.sum(~np.isnan(merged))
        print(f"✅ Maximum merge: {valid_pixels} valid pixels")
        
        return merged
    
    def _regrid_data(self, source_data: np.ndarray, source_coords: Dict[str, np.ndarray],
                    target_extent: Dict[str, np.ndarray], target_shape: Tuple[int, int]) -> Optional[np.ndarray]:
        """Fallback regridding using basic interpolation"""
        try:
            from scipy.interpolate import RegularGridInterpolator
            
            print(f"📉 Using scipy interpolation fallback: {source_data.shape} → {target_shape}")
            
            # Get coordinate arrays
            source_lats = source_coords['lats']
            source_lons = source_coords['lons']
            target_lats = target_extent['lats']
            target_lons = target_extent['lons']
            
            # Ensure data is in correct orientation
            if source_lats[0] > source_lats[-1]:
                source_lats = source_lats[::-1]
                source_data = source_data[::-1, :]
            
            if source_lons[0] > source_lons[-1]:
                source_lons = source_lons[::-1]
                source_data = source_data[:, ::-1]
            
            # Create interpolator
            interpolator = RegularGridInterpolator(
                (source_lats, source_lons), 
                source_data,
                method='linear',
                bounds_error=False,
                fill_value=np.nan
            )
            
            # Create target grid
            target_lat_grid, target_lon_grid = np.meshgrid(target_lats, target_lons, indexing='ij')
            target_points = np.stack([target_lat_grid.ravel(), target_lon_grid.ravel()], axis=-1)
            
            # Interpolate
            interpolated = interpolator(target_points).reshape(target_shape)
            
            print(f"✅ Scipy interpolation complete, valid pixels: {np.sum(np.isfinite(interpolated))}")
            return interpolated
            
        except ImportError:
            print("❌ Scipy not available for fallback interpolation")
            return None
        except Exception as e:
            print(f"❌ Scipy interpolation failed: {e}")
            return None